% !TEX root = /Users/zhuzhuangdi/Desktop/MSUCourses/MachineLearning847/17spr_wang_zhu_du/Final/final_report.tex
\section{Problem Description}
\subsection{Motivation}  
%
In this project, we propose and evaluate different approaches to automatically generate Chinese poems. 
%
Ci are one of the most important genres of Chinese classical poetry. 
%
As a precious cultural heritage, not many of them have been passed down onto the current generation.
%
Therefore, the study of automatic generation of Ci is meaningful, not only because it supplements entertainment and education resources to modern society, but also because it demonstrates the feasibility of applying artificial intelligence in Art generation. 
%

\subsection{Background}
Song Ci is a precious cultural heritage in China, which refers to Classical Chinese poetry typical of the Song dynasty.
%
It arose with the so-called banquet music in Tang dynasty and reached its peak one hundred years later, as a major alternative to Shi poetry\cite{cai2008chinesepoetry} .


Derived from the structure used in Tang poetry, Ci follows more complex and strict rules.
%
There are more than 800 genres for Ci, which is called Cipai\cite{wikici}. 
%
Each Cipai determine the number of characters for different lines, the arrangement of rhyme, and even the location of tones.
%
To create a Song Ci in a specified Cipai, authors need to fill in the words according to the rule matrix associated with that Cipai.
 %
 The uneven lines in Ci follow more continuous syntax consistency than traditional Chinese Tang poetry\cite{cai2008chinesepoetry}.
 
These complex rules for Song Ci make it difficult for AI systems to generate Song Ci with good peropery on structure or meaning consistency.
%
Besides, compared with Tang poetry, the number of available Song Ci in a specified Cipai is relatively small\cite{}, which means we have limited numbers of training data to build any model. 


\subsection{Proposed Approaches} 
%
We propose one traditional approach, and two deep-learning approaches to generate Song Ci, respectively.
%
For traditional approach, we use Genetic Algorithms (GAs).
%
 \wei{More contents here.}
%%%%%%
For deep learning approaches, our first model is to use a Recurrent Neural Network  with Long Short Term Memory (LSTM) units. 
%
We train a RNN model by feeding sentences of SongCi as the input, and ask the RNN model to generate the probability distribution of the next character in the sentence, given the sequence of previous characters.
%
Especially, to capture the long term semantic dependencies between characters in Song Ci,
%
we apply Long Short Term Memory units in the RNN model. 
%%%%%
Another deep learning approach is to use Generative Adversarial Networks (GANs).
%
\nan{More content here}.
%
We will compare the results generated by different approaches with respect to the structure, rhythmic and semantic consistency.
 
\subsection{Technical Challenges and Proposed Solutions} 
The first challenge to build a general model for all types of Song Ci.
%
Different from Shi poetry whose structure is strict,  Song Ci has more than 800 set of Cipai, and different Cipai follows different structural or rhythmic patterns.
%
Therefore, it is difficult to generalize templates or rules for all the Song Ci from limited training dataset.
% 
Our solution is to create a model based on Recurrent Neural Network. For every line generated in the SongCi, its probability is based on the probability of all previously lines.
%
So that the grammatical and rhythmic rules can be automatically captured.


The second challenge is to extract useful features from our training corpus.
%
Inappropriate feature extraction approach may lose the semantic meaning of each character which leads to meaningless outputs from the system.
%
Our solution is to build a vector space model to pre-process the poetry corpus. We first tokenize each Chinese character in the corpus, and then represent each character as a vector which retains the semantic relations among different characters. Therefore, characters with similar meanings have smaller distance in the vector space, while characters that are irrelevant in meanings have larger distance. 


The third challenge is to maintain consistent and poetic meanings throughout the generated SongCi.
%
Compared with Shi poetry, Song Ci are much longer in length and therefore more complicated in context.
%
It is difficult to keep long-distance memory using conventional RNN.
% 
Our solution is to use a Long Short Term Memory (LSTM) model that can track the long-distance semantic information automatically. 